from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
from huggingface_hub import login  # âœ… Hugging Face ë¡œê·¸ì¸ ê¸°ëŠ¥ ì¶”ê°€
import torch
import re
import json
import os

# âœ… í™˜ê²½ ë³€ìˆ˜ì—ì„œ Hugging Face Token ë¶ˆëŸ¬ì˜¤ê¸°
token = os.environ.get("HUGGINGFACE_HUB_TOKEN")
if token:
    login(token=token)
else:
    print("[WARNING] HUGGINGFACE_HUB_TOKEN not set. Model loading may fail.")

MODEL_ID = "google/gemma-2b-it"
device = torch.device("cpu")

# âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(device)
generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)

def call_gemma(prompt: str, max_tokens: int = 512) -> str:
    result = generator(prompt, max_new_tokens=max_tokens, temperature=0.7, do_sample=True)[0]["generated_text"]
    return result

def extract_json(text: str) -> dict:
    match = re.search(r"\{[\s\S]+?\}", text)
    if not match:
        print("[ERROR] JSON í˜•ì‹ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë¬¸ ì¶œë ¥:")
        print(text)
        return {}

    try:
        return json.loads(match.group())
    except json.JSONDecodeError as e:
        print(f"[ERROR] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        print("[ì›ë¬¸]:")
        print(match.group())

        # ğŸ’¡ ë³´ì • ì‹œë„: ë§ˆì§€ë§‰ ê´„í˜¸ê°€ ë¶€ì¡±í•œ ê²½ìš°
        fixed = match.group().strip()
        while not fixed.endswith("}"):
            fixed += "}"
        try:
            return json.loads(fixed)
        except:
            return {}
